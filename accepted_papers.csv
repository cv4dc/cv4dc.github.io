poster,session,number,title,authors,abstract,forum
Proceeding Track,Oral,1,"Nayana: A Foundation for Document-Centric Vision-Language Models via Multi-Task, Multimodal, and Multilingual Data Synthesis","Adithya S Kolavi, Vyoman Jain, Samarth P","We present Nayana, a comprehensive, synthetically generated dataset designed to advance document-centric vision language models across multiple tasks and languages. Nayana consists of three interconnected subsets, each targeting different aspects of document understanding: (1) a base dataset of 3 million document images with hierarchical annotations including detailed layout information, textual content, reading order, and relationships between document elements; (2) a multilingual variant spanning 22 languages, preserving the original document layout while translating text through contextual models; and (3) a specialized information retrieval subset for document ranking tasks with approximately 250,000 image-query pairs per language. What distinguishes Nayana is its synthetic generation methodology. We collect a diverse corpus of PDFs from multiple sources, then apply state-of-the-art models to hierarchically extract structural and textual information, yielding a highly structured representation capturing layout elements, text lines, images, captions, and their interrelationships. For the multilingual extension, we employ contextual translation models to transform textual elements while preserving stylistic and visual attributes. Beyond the primary subsets, Nayana incorporates Visual Question Answering (VQA) pairs in both monolingual and multilingual settings. This multifaceted approach makes Nayana a truly multi-task dataset, enabling training of vision-language models for diverse applications including layout detection, equation recognition, image captioning, markdown conversion, multilingual OCR, document retrieval, and more.",https://openreview.net/forum?id=15sfcTLKYT
Proceeding Track,Oral,2,"Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception","Moseli Mots'oehli, Feimei Chen, Hok Wai Chan, Itumeleng Victor Tlali, Thulani Babeli, Kyungim Baek, Huaijin Chen","The lack of publicly available autonomous vehicle datasets from developing regions—particularly across diverse African road environments including urban, rural, and unpaved terrain—hampers progress in robust perception for low-resource settings. We introduce a procedural augmentation pipeline that enriches low-cost monocular dashcam footage with realistic refractive distortions and weather-induced artifacts tailored to these challenging African driving scenarios. Our refractive module simulates distortions from low-quality lenses and air turbulence, including lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free (incompressible) warps. The weather module adds homogeneous fog, heterogeneous fog, and lens flare. To support autonomous perception research in underrepresented African contexts—without the need for costly data collection, labeling, or simulation—we release our distortion toolkit and augmented dataset splits along with the baseline restoration model.",https://openreview.net/forum?id=1M1uWcMPm0
Proceeding Track,Oral,3,"V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using the iRAP Standard?","Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Sriroongvikrai, Nicola Christie, Meihui Wang, Huanfa Chen, James Haworth","Road safety assessments are critical yet costly, especially in Low- and Middle-Income Countries (LMICs), where most roads remain unrated. Traditional methods require expert annotation and training data, while supervised learning-based approaches struggle to generalise across regions. In this paper, we introduce \textit{V-RoAst}, a zero-shot Visual Question Answering (VQA) framework using Vision-Language Models (VLMs) to classify road safety attributes defined by the iRAP standard. We introduce the first open-source dataset from ThaiRAP, consisting of over 2,000 curated street-level images from Thailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini on this dataset and benchmark their performance against VGGNet and ResNet baselines. While VLMs underperform on spatial awareness, they generalise well to unseen classes and offer flexible prompt-based reasoning without retraining. Our results show that VLMs can serve as automatic road assessment tools when integrated with complementary data. This work is the first to explore VLMs for zero-shot infrastructure risk assessment and opens new directions for automatic, low-cost road safety mapping",https://openreview.net/forum?id=RvERGUYOIz
Proceeding Track,Oral,4,"FLD+: Data-efficient Evaluation Metric for Generative Models","Pranav Jeevan P, Neeraj Nixon, Amit Sethi","We introduce a new metric to assess the quality of generated images that is more reliable, data-efficient, compute-efficient, and adaptable to new domains than the previous metrics, such as Fréchet Inception Distance (FID). The proposed metric is based on normalizing flows, which allows for the computation of density (exact log-likelihood) of images from any domain. Thus, unlike FID, the proposed Flow-based Likelihood Distance Plus (FLD+) metric exhibits strongly monotonic behavior with respect to different types of image degradations, including noise, occlusion, diffusion steps, and generative model size. Additionally, because normalizing flow can be trained stably and efficiently, FLD+ achieves stable results with two orders of magnitude fewer images than FID (which requires more images to reliably compute Fréchet distance between features of large samples of real and generated images). We made FLD+ computationally even more efficient by applying normalizing flows to features extracted in a lower-dimensional latent space instead of using a pre-trained network. We also show that FLD+ can easily be retrained on new domains, such as medical images, unlike the networks behind previous metrics -- such as InceptionNetV3 pre-trained on ImageNet.",https://openreview.net/forum?id=erswAjtQ1i
Proceeding Track,Oral,5,"WavePaint: Resource-efficient Token-mixer for Self-supervised Inpainting","Pranav Jeevan P, Dharshan Sampath Kumar, Amit Sethi","Inpainting, which refers to the synthesis of missing regions, can help restore occluded or degraded areas of an image and also serve as a precursor task for self-supervision of neural networks for computer vision. The current state-of-the-art models for inpainting are computationally heavy as they are based on transformer or CNN backbones that are trained in adversarial or diffusion settings. This paper diverges from vision transformers by using a computationally-efficient WaveMix-based fully convolutional architecture---WavePaint. It uses a 2D-discrete wavelet transform (DWT) for spatial and multi-resolution token-mixing along with convolutional layers. The proposed model outperforms the current state-of-the-art models for image inpainting on reconstruction quality while also using much fewer parameters and GPU RAM, and considerably lower training and evaluation times. Our model even outperforms current GAN-based architectures in CelebA-HQ dataset without using an adversarially trainable discriminator. This work suggests that neural architectures that are modeled after natural image priors require fewer parameters and computations to achieve better generalization.",https://openreview.net/forum?id=u0uyCaBFAW
,,,,,,
Poster 1,,7,title 7,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,8,title 8,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,9,title 9,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,10,title 10,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,11,title 11,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,12,title 12,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,13,title 13,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,14,title 14,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,15,title 15,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,16,title 16,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,17,title 17,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,18,title 18,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,19,title 19,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,20,title 20,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,21,title 21,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,22,title 22,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,23,title 23,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 1,,24,title 24,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 2,,25,title 25,"author 1, author 2",this is the abstract,https://openreview.net/
Poster 3,,26,title 26,"author 1, author 2",this is the abstract,https://openreview.net/
